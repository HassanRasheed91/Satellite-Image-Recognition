{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_7yorxZHARx",
    "outputId": "304ec393-b93d-42f0-f816-915d4a85bf9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.12.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HnnNripJHD9Q",
    "outputId": "1fc75328-6afd-41cd-a90f-00817d35fdba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website Open Ho Gayi!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # GUI nahi hai is liye headless mode chalayenge\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(\"https://www.gps-coordinates.net/satellite\")\n",
    "\n",
    "print(\"Website Open Ho Gayi!\")  # Check ke liye\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eizNUjFU72PB",
    "outputId": "cbdb59df-6568-44f5-fad8-250bbfebbed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 31.3, 74.2\n",
      "Zoom error: Message: element not interactable\n",
      "  (Session info: chrome=133.0.6943.127)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF67CBF6EE5+28773]\n",
      "\t(No symbol) [0x00007FF67CB625D0]\n",
      "\t(No symbol) [0x00007FF67C9F8DDC]\n",
      "\t(No symbol) [0x00007FF67CA506B4]\n",
      "\t(No symbol) [0x00007FF67CA42284]\n",
      "\t(No symbol) [0x00007FF67CA7724A]\n",
      "\t(No symbol) [0x00007FF67CA41B36]\n",
      "\t(No symbol) [0x00007FF67CA77460]\n",
      "\t(No symbol) [0x00007FF67CA9F6F3]\n",
      "\t(No symbol) [0x00007FF67CA77023]\n",
      "\t(No symbol) [0x00007FF67CA3FF5E]\n",
      "\t(No symbol) [0x00007FF67CA411E3]\n",
      "\tGetHandleVerifier [0x00007FF67CF4422D+3490733]\n",
      "\tGetHandleVerifier [0x00007FF67CF5BA13+3586963]\n",
      "\tGetHandleVerifier [0x00007FF67CF5144D+3544525]\n",
      "\tGetHandleVerifier [0x00007FF67CCBC9AA+838442]\n",
      "\t(No symbol) [0x00007FF67CB6D01F]\n",
      "\t(No symbol) [0x00007FF67CB695E4]\n",
      "\t(No symbol) [0x00007FF67CB69786]\n",
      "\t(No symbol) [0x00007FF67CB58CB9]\n",
      "\tBaseThreadInitThunk [0x00007FFC42DDE8D7+23]\n",
      "\tRtlUserThreadStart [0x00007FFC4437BF2C+44]\n",
      "\n",
      "Saved: lahore_images/lahore_image_0.png\n",
      "Processing 31.3, 74.25\n"
     ]
    },
    {
     "ename": "ElementClickInterceptedException",
     "evalue": "Message: element click intercepted: Element <button draggable=\"false\" aria-label=\"Toggle fullscreen view\" title=\"Toggle fullscreen view\" type=\"button\" aria-pressed=\"false\" class=\"gm-control-active gm-fullscreen-control\" style=\"background: none rgb(255, 255, 255); border: 0px; margin: 10px; padding: 0px; text-transform: none; appearance: none; position: absolute; cursor: pointer; user-select: none; border-radius: 2px; height: 40px; width: 40px; box-shadow: rgba(0, 0, 0, 0.3) 0px 1px 4px -1px; overflow: hidden; top: 0px; right: 0px;\">...</button> is not clickable at point (949, 86). Other element would receive the click: <ul class=\"nav nav-tabs\">...</ul>\n  (Session info: chrome=133.0.6943.127)\nStacktrace:\n\tGetHandleVerifier [0x00007FF67CBF6EE5+28773]\n\t(No symbol) [0x00007FF67CB625D0]\n\t(No symbol) [0x00007FF67C9F8FAA]\n\t(No symbol) [0x00007FF67CA57109]\n\t(No symbol) [0x00007FF67CA54AD2]\n\t(No symbol) [0x00007FF67CA51B81]\n\t(No symbol) [0x00007FF67CA50A91]\n\t(No symbol) [0x00007FF67CA42284]\n\t(No symbol) [0x00007FF67CA7724A]\n\t(No symbol) [0x00007FF67CA41B36]\n\t(No symbol) [0x00007FF67CA77460]\n\t(No symbol) [0x00007FF67CA9F6F3]\n\t(No symbol) [0x00007FF67CA77023]\n\t(No symbol) [0x00007FF67CA3FF5E]\n\t(No symbol) [0x00007FF67CA411E3]\n\tGetHandleVerifier [0x00007FF67CF4422D+3490733]\n\tGetHandleVerifier [0x00007FF67CF5BA13+3586963]\n\tGetHandleVerifier [0x00007FF67CF5144D+3544525]\n\tGetHandleVerifier [0x00007FF67CCBC9AA+838442]\n\t(No symbol) [0x00007FF67CB6D01F]\n\t(No symbol) [0x00007FF67CB695E4]\n\t(No symbol) [0x00007FF67CB69786]\n\t(No symbol) [0x00007FF67CB58CB9]\n\tBaseThreadInitThunk [0x00007FFC42DDE8D7+23]\n\tRtlUserThreadStart [0x00007FFC4437BF2C+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementClickInterceptedException\u001b[0m          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m scraper\u001b[38;5;241m.\u001b[39minitiate_browser()\n\u001b[0;32m    102\u001b[0m lahore_coordinates \u001b[38;5;241m=\u001b[39m generate_lahore_coordinates()\n\u001b[1;32m--> 103\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlahore_coordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlahore_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m scraper\u001b[38;5;241m.\u001b[39mquit_browser()\n",
      "Cell \u001b[1;32mIn[4], line 68\u001b[0m, in \u001b[0;36mGoogleEarthScraper.scrape\u001b[1;34m(self, coordinates_list, output_folder)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbutton[onclick=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodeLatLng(1)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     67\u001b[0m max_btn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39melement_to_be_clickable((By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//button[@aria-pressed=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m---> 68\u001b[0m \u001b[43mmax_btn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust_zoom()\n\u001b[0;32m     72\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:119\u001b[0m, in \u001b[0;36mWebElement.click\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclick\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clicks the element.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    >>> element.click()\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLICK_ELEMENT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:572\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    570\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    571\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mElementClickInterceptedException\u001b[0m: Message: element click intercepted: Element <button draggable=\"false\" aria-label=\"Toggle fullscreen view\" title=\"Toggle fullscreen view\" type=\"button\" aria-pressed=\"false\" class=\"gm-control-active gm-fullscreen-control\" style=\"background: none rgb(255, 255, 255); border: 0px; margin: 10px; padding: 0px; text-transform: none; appearance: none; position: absolute; cursor: pointer; user-select: none; border-radius: 2px; height: 40px; width: 40px; box-shadow: rgba(0, 0, 0, 0.3) 0px 1px 4px -1px; overflow: hidden; top: 0px; right: 0px;\">...</button> is not clickable at point (949, 86). Other element would receive the click: <ul class=\"nav nav-tabs\">...</ul>\n  (Session info: chrome=133.0.6943.127)\nStacktrace:\n\tGetHandleVerifier [0x00007FF67CBF6EE5+28773]\n\t(No symbol) [0x00007FF67CB625D0]\n\t(No symbol) [0x00007FF67C9F8FAA]\n\t(No symbol) [0x00007FF67CA57109]\n\t(No symbol) [0x00007FF67CA54AD2]\n\t(No symbol) [0x00007FF67CA51B81]\n\t(No symbol) [0x00007FF67CA50A91]\n\t(No symbol) [0x00007FF67CA42284]\n\t(No symbol) [0x00007FF67CA7724A]\n\t(No symbol) [0x00007FF67CA41B36]\n\t(No symbol) [0x00007FF67CA77460]\n\t(No symbol) [0x00007FF67CA9F6F3]\n\t(No symbol) [0x00007FF67CA77023]\n\t(No symbol) [0x00007FF67CA3FF5E]\n\t(No symbol) [0x00007FF67CA411E3]\n\tGetHandleVerifier [0x00007FF67CF4422D+3490733]\n\tGetHandleVerifier [0x00007FF67CF5BA13+3586963]\n\tGetHandleVerifier [0x00007FF67CF5144D+3544525]\n\tGetHandleVerifier [0x00007FF67CCBC9AA+838442]\n\t(No symbol) [0x00007FF67CB6D01F]\n\t(No symbol) [0x00007FF67CB695E4]\n\t(No symbol) [0x00007FF67CB69786]\n\t(No symbol) [0x00007FF67CB58CB9]\n\tBaseThreadInitThunk [0x00007FFC42DDE8D7+23]\n\tRtlUserThreadStart [0x00007FFC4437BF2C+44]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class GoogleEarthScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.wait = None\n",
    "        self.zoomed = False\n",
    "\n",
    "    def initiate_browser(self):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--headless\")  # Background mode\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.wait = WebDriverWait(self.driver, 20)\n",
    "        self.driver.get('https://www.gps-coordinates.net/satellite')\n",
    "        self.driver.maximize_window()\n",
    "\n",
    "        try:\n",
    "            pop_up = self.driver.find_element(By.CSS_SELECTOR, '#CybotCookiebotDialogBodyButtonDecline')\n",
    "            pop_up.click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    def take_screenshot(self, img_path):\n",
    "        time.sleep(2)  # Ensure page is fully rendered\n",
    "        self.driver.save_screenshot(img_path)\n",
    "\n",
    "    def adjust_zoom(self):\n",
    "        try:\n",
    "            zoom_in = self.driver.find_element(By.CSS_SELECTOR, \"button[aria-label$='in']\")\n",
    "\n",
    "            # Increase zoom to capture a closer view\n",
    "            for _ in range(5):  # Increased zoom level\n",
    "                zoom_in.click()\n",
    "                time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Zoom error: {e}\")\n",
    "\n",
    "    def scrape(self, coordinates_list, output_folder):\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        for index, coordinates in enumerate(coordinates_list):\n",
    "            lat, long = coordinates\n",
    "            print(f\"Processing {lat}, {long}\")\n",
    "\n",
    "            lat_field = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#latitude\")))\n",
    "            lat_field.clear()\n",
    "            lat_field.send_keys(lat)\n",
    "\n",
    "            long_key = self.driver.find_element(By.CSS_SELECTOR, \"#longitude\")\n",
    "            long_key.clear()\n",
    "            long_key.send_keys(long)\n",
    "\n",
    "            self.driver.find_element(By.CSS_SELECTOR, \"button[onclick='codeLatLng(1)']\").click()\n",
    "\n",
    "            max_btn = self.wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@aria-pressed='false']\")))\n",
    "            max_btn.click()\n",
    "\n",
    "            self.adjust_zoom()\n",
    "\n",
    "            time.sleep(5)\n",
    "            img_path = f\"{output_folder}/lahore_image_{index}.png\"\n",
    "            self.take_screenshot(img_path)\n",
    "            max_btn.click()\n",
    "\n",
    "            print(f\"Saved: {img_path}\")\n",
    "\n",
    "    def quit_browser(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "# Generate GPS coordinates for Lahore\n",
    "\n",
    "def generate_lahore_coordinates():\n",
    "    lat_start, lat_end = 31.3, 31.7  # Lahore Latitude Range\n",
    "    lon_start, lon_end = 74.2, 74.6  # Lahore Longitude Range\n",
    "    step = 0.05  # Step size for grid\n",
    "\n",
    "    coordinates = []\n",
    "    lat = lat_start\n",
    "    while lat <= lat_end:\n",
    "        lon = lon_start\n",
    "        while lon <= lon_end:\n",
    "            coordinates.append((str(round(lat, 4)), str(round(lon, 4))))\n",
    "            lon += step\n",
    "        lat += step\n",
    "    return coordinates\n",
    "\n",
    "# Example Usage:\n",
    "scraper = GoogleEarthScraper()\n",
    "scraper.initiate_browser()\n",
    "lahore_coordinates = generate_lahore_coordinates()\n",
    "scraper.scrape(lahore_coordinates, \"lahore_images\")\n",
    "scraper.quit_browser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-Tff_dhllf-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fevC0hAvmSH7"
   },
   "outputs": [],
   "source": [
    "#Image Augmentation Functions\n",
    "# Flip Functions\n",
    "def horizontal_flip(image):\n",
    "    return cv2.flip(image, 1)\n",
    "\n",
    "def vertical_flip(image):\n",
    "    return cv2.flip(image, 0)\n",
    "\n",
    "def mirror(image):\n",
    "    return cv2.flip(image, -1)\n",
    "\n",
    "# Rotation Function\n",
    "def rotate(image, angle):\n",
    "    rows, cols, _ = image.shape\n",
    "    matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "    return cv2.warpAffine(image, matrix, (cols, rows))\n",
    "\n",
    "# Cropping Function\n",
    "def crop(image, x, y, w, h):\n",
    "    return image[y:y+h, x:x+w]\n",
    "\n",
    "# Resizing Function\n",
    "def resize(image, width, height):\n",
    "    return cv2.resize(image, (width, height))\n",
    "\n",
    "# Color Jitter Function (Brightness, Contrast, Saturation, Hue)\n",
    "def color_jitter(image, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    hsv[..., 2] = hsv[..., 2] * (1 + brightness * np.random.uniform(-1, 1))\n",
    "    hsv[..., 2] = hsv[..., 2] * (1 + contrast * np.random.uniform(-1, 1))\n",
    "    hsv[..., 1] = hsv[..., 1] * (1 + saturation * np.random.uniform(-1, 1))\n",
    "\n",
    "    hsv[..., 0] = hsv[..., 0] + hue * np.random.uniform(-1, 1) * 360\n",
    "    hsv[..., 0] = np.clip(hsv[..., 0], 0, 360)\n",
    "\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "# Add Gaussian Noise\n",
    "def add_gaussian_noise(image, mean=0, sigma=25):\n",
    "    row, col, ch = image.shape\n",
    "    gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "    noisy = image + gauss\n",
    "    return np.clip(noisy, 0, 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJoLrb12nBVo"
   },
   "source": [
    "# Step 3: Image Processing Pipeline\n",
    "Yeh function ek image par multiple augmentations apply karega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iC-1EcSlllNJ"
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    augmented_images = [\n",
    "        horizontal_flip(image),\n",
    "        vertical_flip(image),\n",
    "        resize(image, 750, 500),  # Resize before cropping\n",
    "        crop(resize(image, 750, 500), 50, 50, 1500, 600),\n",
    "        add_gaussian_noise(image),\n",
    "        color_jitter(image)\n",
    "    ]\n",
    "    return augmented_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB5VLXjAnP0m"
   },
   "source": [
    "# Step 4: Apply Augmentation on All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "95b29267ac1d4b7d888a66f8907edfd6",
      "debb2111c66142e9a6ad0a5e5925079b",
      "112db7367b9a4250a6cb5bf0a7a1ba81",
      "a7f70e45e8974befb7f65b1d610ffee1",
      "f261a885f1a245c7bae05b7b03dbc363",
      "1367bdb96b114166aa505ac8a3919c6a",
      "e5cdec9f7c0b4a29955a52123d3874d5",
      "df31bf99984f45f59322fbbe2e55fb71",
      "c161b69a6a2a4f9bad63ed15a0130c55",
      "2647c2be4a824fffaab8848c5e0e0949",
      "713cb400e07b48a8a9a19029a700302d"
     ]
    },
    "id": "oMtjE1pD-LM9",
    "outputId": "f4c7f991-7a03-49ef-9ecf-2326e2eaebe7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "# Image Augmentation Functions\n",
    "def horizontal_flip(image):\n",
    "    return cv2.flip(image, 1)\n",
    "\n",
    "def vertical_flip(image):\n",
    "    return cv2.flip(image, 0)\n",
    "\n",
    "def mirror(image):\n",
    "    return cv2.flip(image, -1)\n",
    "\n",
    "def rotate(image, angle):\n",
    "    rows, cols, _ = image.shape\n",
    "    matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "    return cv2.warpAffine(image, matrix, (cols, rows))\n",
    "\n",
    "def crop(image, x, y, w, h):\n",
    "    return image[y:y+h, x:x+w]\n",
    "\n",
    "def resize(image, width, height):\n",
    "    return cv2.resize(image, (width, height))\n",
    "\n",
    "def color_jitter(image, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    hsv[..., 2] = hsv[..., 2] * (1 + brightness * np.random.uniform(-1, 1))\n",
    "    hsv[..., 2] = hsv[..., 2] * (1 + contrast * np.random.uniform(-1, 1))\n",
    "    hsv[..., 1] = hsv[..., 1] * (1 + saturation * np.random.uniform(-1, 1))\n",
    "\n",
    "    hsv[..., 0] = hsv[..., 0] + hue * np.random.uniform(-1, 1) * 360\n",
    "    hsv[..., 0] = np.clip(hsv[..., 0], 0, 360)\n",
    "\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, sigma=25):\n",
    "    row, col, ch = image.shape\n",
    "    gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "    noisy = image + gauss\n",
    "    return np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "\n",
    "def process_image(image):\n",
    "    augmented_images = [\n",
    "        horizontal_flip(image),\n",
    "        vertical_flip(image),\n",
    "        resize(image, 750, 500),  # Resize before cropping\n",
    "        #crop(resize(image, 750, 500), 50, 50, 1500, 600), # This line causes error due to inappropriate crop dimensions\n",
    "        add_gaussian_noise(image),\n",
    "        color_jitter(image)\n",
    "    ]\n",
    "    return augmented_images\n",
    "\n",
    "def save_image(image, output_path, prefix, original_name):\n",
    "    filename = f\"{prefix}_{original_name}\"\n",
    "    filepath = os.path.join(output_path, filename)\n",
    "    cv2.imwrite(filepath, image)\n",
    "    print(f\"✅ Saved: {filename}\")\n",
    "\n",
    "def apply_augmentations(input_path_, output_path_):\n",
    "    if not os.path.exists(output_path_):\n",
    "        os.makedirs(output_path_)\n",
    "\n",
    "    for filename in tqdm(os.listdir(input_path_)):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(input_path_, filename)\n",
    "            original_image = cv2.imread(image_path)\n",
    "\n",
    "            if original_image is None:\n",
    "                continue\n",
    "\n",
    "            augmented_images = process_image(original_image)\n",
    "\n",
    "            for i, aug_img in enumerate(augmented_images):\n",
    "                save_image(aug_img, output_path_, f\"aug{i+1}\", filename)\n",
    "\n",
    "# Device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load ResNet50 Model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove the last layer (fc) and use the features before it\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # ResNet50 input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    return input_tensor\n",
    "\n",
    "# Feature Extraction Function\n",
    "def extract_features(image_path, model):\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model(image_tensor)\n",
    "\n",
    "    features = features.cpu().numpy().flatten()  # Convert to numpy\n",
    "    return features\n",
    "\n",
    "# Extract Features from All Images in the Folder\n",
    "def extract_features_from_folder(folder_path, model):\n",
    "    all_features = []\n",
    "    image_names = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):  # Sirf image files par kaam karein\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            features = extract_features(image_path, model)\n",
    "            all_features.append(features)\n",
    "            image_names.append(filename)\n",
    "\n",
    "    return np.array(all_features), image_names\n",
    "\n",
    "input_folder = \"/content/lahore_images/\"\n",
    "output_folder = \"/content/Augment_images/\"\n",
    "\n",
    "apply_augmentations(input_folder, output_folder)\n",
    "\n",
    "folder_path = output_folder  # Use the augmented images folder\n",
    "features_array, image_files = extract_features_from_folder(folder_path, model)\n",
    "np.save(\"/content/extracted_features.npy\", features_array)\n",
    "np.save(\"/content/image_names.npy\", image_files)\n",
    "\n",
    "# Rest of your code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJYQNXevnbRw"
   },
   "source": [
    "# Step 5: Saving Augmented Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAgVHTCtriCt"
   },
   "source": [
    "#  Feature Extraction with ResNet50\n",
    "🔹 Ye code satellite image se features extract karega aur unko numpy array me convert karega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWeCDcWUrhYW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ✅ GPU Check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Load ResNet50 Model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # Last classification layer hata di\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ✅ Image Preprocessing Function\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # ResNet50 input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    return input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLerztbzrhU6"
   },
   "outputs": [],
   "source": [
    "# ✅ Feature Extraction Function\n",
    "def extract_features(image_path, model):\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model(image_tensor)\n",
    "\n",
    "    features = features.cpu().numpy().flatten()  # Convert to numpy\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOLnuzdJrhLH"
   },
   "outputs": [],
   "source": [
    "# ✅ Extract Features from All Images in the Folder\n",
    "def extract_features_from_folder(folder_path, model):\n",
    "    all_features = []\n",
    "    image_names = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):  # Sirf image files par kaam karein\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            features = extract_features(image_path, model)\n",
    "            all_features.append(features)\n",
    "            image_names.append(filename)\n",
    "\n",
    "    return np.array(all_features), image_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRZBgWJPrhHt"
   },
   "outputs": [],
   "source": [
    "# ✅ Folder Path Select Karein\n",
    "folder_path = \"/content/Augment_images\"  # Ya augmented images ka folder\n",
    "\n",
    "# ✅ Extract Features\n",
    "features_array, image_files = extract_features_from_folder(folder_path, model)\n",
    "\n",
    "# ✅ Save Features to Numpy File\n",
    "np.save(\"/content/extracted_features.npy\", features_array)\n",
    "\n",
    "#np.save(\"/content/drive/MyDrive/extracted_features.npy\", features_array)\n",
    "\n",
    "print(\"Feature Extraction Completed! Features Shape:\", features_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqhuKO__rhD9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# ✅ Extracted Features Load Karo\n",
    "features = np.load(\"/content/extracted_features.npy\")\n",
    "\n",
    "# ✅ K-Means Clustering Apply Karo (5 Classes: Water, Desert, Industrial, Barren, Crop)\n",
    "K = 5\n",
    "kmeans = KMeans(n_clusters=K, random_state=42)\n",
    "labels = kmeans.fit_predict(features)\n",
    "\n",
    "# ✅ Save Cluster Labels\n",
    "np.save(\"/content/cluster_labels.npy\", labels)\n",
    "joblib.dump(kmeans, \"/content/kmeans_model.pkl\")\n",
    "\n",
    "# ✅ Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Save Training Data\n",
    "np.save(\"/content/X_train.npy\", X_train)\n",
    "np.save(\"/content/X_test.npy\", X_test)\n",
    "np.save(\"/content/y_train.npy\", y_train)\n",
    "np.save(\"/content/y_test.npy\", y_test)\n",
    "\n",
    "print(\"✅ Clustering Completed! Labels Assigned and Dataset Saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBL8CW_53f4P"
   },
   "source": [
    "#ResNet-Based Fine-Tuned Model for Classification\n",
    "Ab hum pre-trained ResNet50 ko fine-tune karenge taake satellite image classification (Water, Desert, Industrial, Barren, Crop) ke liye best results milein. Fine-tuning ka matlab hai ke hum ResNet ka feature extraction part use karenge aur sirf last layers ko retrain karenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh83La3J3kcI"
   },
   "source": [
    "# Steps:\n",
    "Dataset Load Karo (X_train.npy, y_train.npy)\n",
    "\n",
    "PyTorch Dataset & Dataloader Banayo\n",
    "\n",
    "ResNet50 ka Feature Extractor Use Karo (Pretrained)\n",
    "\n",
    "Fully Connected Layer Replace Karo (5 Classes ke liye)\n",
    "\n",
    "Model Train & Validate Karo\n",
    "Trained Model Save Karo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owyU0ddRnVTX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ✅ GPU Check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using Device:\", device)\n",
    "\n",
    "# ✅ Load Features & Labels\n",
    "X_train = np.load(\"/content/X_train.npy\")\n",
    "X_test = np.load(\"/content/X_test.npy\")\n",
    "y_train = np.load(\"/content/y_train.npy\")\n",
    "y_test = np.load(\"/content/y_test.npy\")\n",
    "\n",
    "# ✅ Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# ✅ Create PyTorch Dataset & DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ✅ Load Pretrained ResNet50 Model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "# Remove the last layer (fc) and use the features before it\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet.to(device)\n",
    "\n",
    "# ✅ Custom Classifier Layer\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes=5):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.fc = nn.Linear(2048, num_classes)  # 2048 features → 5 Classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  Pass the features (x) directly to the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten the features\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "# ✅ Initialize Model\n",
    "model = ResNetClassifier(resnet, num_classes=5).to(device)\n",
    "\n",
    "# ✅ Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# ✅ Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Pass the inputs (features) to the model\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# ✅ Save Model\n",
    "torch.save(model.state_dict(), \"/content/resnet_finetuned.pth\")\n",
    "print(\"✅ Model Training Complete & Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu9h9A5T5w8l"
   },
   "source": [
    "#  ResNet Fine-Tuned Model Test Karna (Prediction Code)\n",
    "Ab hum trained ResNet model ko test karenge aur dekhenge ke yeh new satellite images par Water, Desert, Industrial, Barren, Crop categories ko correctly classify karta hai ya nahi.\n",
    "\n",
    "# Steps:\n",
    "Trained Model Load Karo\n",
    "\n",
    "New Images ka Feature Extract Karo\n",
    "\n",
    "Model se Predict Karo\n",
    "\n",
    "Predicted Labels Dikhao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFII8kbbnVQC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ✅ Device Check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using Device:\", device)\n",
    "\n",
    "# ✅ Load Test Features & Labels\n",
    "X_test = np.load(\"/content/X_test.npy\")\n",
    "y_test = np.load(\"/content/y_test.npy\")\n",
    "\n",
    "# ✅ Convert to PyTorch Tensor\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# ✅ Create DataLoader\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ✅ Load Pretrained ResNet Model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove last classification layer\n",
    "resnet.to(device)\n",
    "\n",
    "# ✅ Define Classifier Model (Same as Training)\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes=5):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the features (x) directly to the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten the features\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "# ✅ Load Trained Model\n",
    "model = ResNetClassifier(resnet, num_classes=5).to(device)\n",
    "model.load_state_dict(torch.load(\"/content/resnet_finetuned.pth\", map_location=device))\n",
    "model.eval()\n",
    "print(\"✅ Model Loaded Successfully!\")\n",
    "\n",
    "# ✅ Load Trained Model\n",
    "model = ResNetClassifier(resnet, num_classes=5).to(device)\n",
    "model.load_state_dict(torch.load(\"/content/resnet_finetuned.pth\", map_location=device))\n",
    "model.eval()\n",
    "print(\"✅ Model Loaded Successfully!\")\n",
    "\n",
    "# ✅ Predict on Test Data\n",
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# ✅ Accuracy Calculation\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"✅ Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# ✅ Save Predictions\n",
    "np.save(\"/content/predicted_labels.npy\", np.array(predictions))\n",
    "print(\"✅ Predictions Saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNrke0Vw7m_f"
   },
   "source": [
    "# Step 1: Confusion Matrix Plot Karo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kkrI6R-5kLP"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ✅ Load Test Labels & Predictions\n",
    "y_test = np.load(\"/content/y_test.npy\")\n",
    "y_pred = np.load(\"/content/predicted_labels.npy\")\n",
    "\n",
    "# ✅ Generate Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ✅ Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Water\", \"Desert\", \"Industrial\", \"Barren\", \"Crop\"],\n",
    "            yticklabels=[\"Water\", \"Desert\", \"Industrial\", \"Barren\", \"Crop\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1Z2DToX78Vb"
   },
   "source": [
    "# Step 2: Misclassified Images Show Karo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grU1q8uH7vt1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# ✅ Load Image Names\n",
    "image_files = np.load(\"/content/image_names.npy\")  # Names of test images\n",
    "\n",
    "# ✅ Find Misclassified Indices\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "\n",
    "# ✅ Randomly Select Some Misclassified Images\n",
    "random_indices = random.sample(list(misclassified_indices), min(10, len(misclassified_indices)))\n",
    "\n",
    "# ✅ Plot Misclassified Images\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Updated path to the correct directory where images were saved\n",
    "    img_path = os.path.join(\"/content/Augment_images\", image_files[idx])\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"True: {y_test[idx]}, Pred: {y_pred[idx]}\")\n",
    "\n",
    "plt.show()  # ✅ Corrected placement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HflCikMGGGnh"
   },
   "source": [
    "# Training Function Add Karo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cj7WQCrQ8F1U"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ✅ Training Function\n",
    "def train_model(model, optimizer, scheduler, train_loader, num_epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss Function\n",
    "    model.train()  # Training Mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step(running_loss)  # Learning Rate Scheduler\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0QdWULxGM2p"
   },
   "source": [
    "# Accuracy Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2L1KfdzqGJ70"
   },
   "outputs": [],
   "source": [
    "# ✅ Accuracy Evaluation Function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"✅ Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPYbRm3GSfL"
   },
   "source": [
    "#  Final Hyperparameter Tuning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwjWHdaCGPy7"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# ✅ Learning Rate Tuning\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "best_lr = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    # ✅ Train Model\n",
    "    model = train_model(model, optimizer, scheduler, train_loader, num_epochs=5)\n",
    "\n",
    "    # ✅ Evaluate Model\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"✅ Best Learning Rate: {best_lr}, Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c9fSKH-HhHE"
   },
   "source": [
    "#  fine-tuned ResNet model ko kisi bhi random image par test karna chahte ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEN-dJ8zGU7Q"
   },
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "\n",
    "# # ✅ Image Preprocessing Function (same as training)\n",
    "# def preprocess_image(image_path):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # ResNet50 input size\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     image = Image.open(image_path).convert('RGB')  # Load Image\n",
    "#     input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "#     return input_tensor.to(device)\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# ✅ Image Preprocessing Function (same as training)\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # ResNet50 input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image_path = '/content/Augment_images/aug1_lahore_image_50.png'\n",
    "    image = Image.open(image_path).convert('RGB')  # Load Image\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Pass the image through the feature extractor (resnet)\n",
    "    with torch.no_grad():\n",
    "        features = resnet(image_tensor.to(device))\n",
    "\n",
    "    # Flatten the features\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    return features # Return the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TJH2ng6Hnen"
   },
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, class_labels):\n",
    "    model.eval()  # Model evaluation mode\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "\n",
    "    _, predicted_class = torch.max(output, 1)  # Get class with highest probability\n",
    "    predicted_label = class_labels[predicted_class.item()]\n",
    "\n",
    "    print(f\"✅ Predicted Class: {predicted_label}\")\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCQ2fW0_HpIp"
   },
   "outputs": [],
   "source": [
    "# ✅ Define Labels (Based on your problem)s\n",
    "class_labels = [\"Water\", \"Desert\", \"Industrial\", \"Barren\", \"Crop\"]\n",
    "\n",
    "# ✅ Random Image Path (Change as needed)\n",
    "test_image_path = \"/content/Augment_images/aug1_lahore_image_50.png\"  # Apni image ka path do\n",
    "\n",
    "# ✅ Test Model on Image\n",
    "predicted_category = predict_image(test_image_path, model, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSZHIvcCIcN-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zu5vQQ8XR5Zo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6ekzebVR5WV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSXXQgB1Opu1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "112db7367b9a4250a6cb5bf0a7a1ba81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df31bf99984f45f59322fbbe2e55fb71",
      "max": 72,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c161b69a6a2a4f9bad63ed15a0130c55",
      "value": 72
     }
    },
    "1367bdb96b114166aa505ac8a3919c6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2647c2be4a824fffaab8848c5e0e0949": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "713cb400e07b48a8a9a19029a700302d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95b29267ac1d4b7d888a66f8907edfd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_debb2111c66142e9a6ad0a5e5925079b",
       "IPY_MODEL_112db7367b9a4250a6cb5bf0a7a1ba81",
       "IPY_MODEL_a7f70e45e8974befb7f65b1d610ffee1"
      ],
      "layout": "IPY_MODEL_f261a885f1a245c7bae05b7b03dbc363"
     }
    },
    "a7f70e45e8974befb7f65b1d610ffee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2647c2be4a824fffaab8848c5e0e0949",
      "placeholder": "​",
      "style": "IPY_MODEL_713cb400e07b48a8a9a19029a700302d",
      "value": " 72/72 [01:05&lt;00:00,  1.21it/s]"
     }
    },
    "c161b69a6a2a4f9bad63ed15a0130c55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "debb2111c66142e9a6ad0a5e5925079b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1367bdb96b114166aa505ac8a3919c6a",
      "placeholder": "​",
      "style": "IPY_MODEL_e5cdec9f7c0b4a29955a52123d3874d5",
      "value": "100%"
     }
    },
    "df31bf99984f45f59322fbbe2e55fb71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5cdec9f7c0b4a29955a52123d3874d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f261a885f1a245c7bae05b7b03dbc363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
